{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "462f01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import json\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c18a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_numeric(df, num_cols):\n",
    "    for col in num_cols:\n",
    "        val = df[col]\n",
    "        if val.min() < 0:\n",
    "            val = val + abs(val.min())\n",
    "        val = val + 1\n",
    "        val = np.log2(val)\n",
    "        val = val + 1\n",
    "        val.fillna(-1, inplace=True)######  заполняем -1 NaNs, все значения не NaNs > 1, а паддинг - 0\n",
    "        df[col]=val\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec278f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73478b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(array, max_len):\n",
    "    shape = array.shape\n",
    "    padds = [(0,0) for _ in range(len(shape) - 1)] + [(0, max_len - shape[0])]\n",
    "    padds = [s for t in padds for s in t]\n",
    "    padded = F.pad(array,padds, \"constant\", 0)\n",
    "    return padded\n",
    "\n",
    "def preprocess_binary(df):\n",
    "    df['B_31'] = df['B_31'].astype(int) + 1 ## теперь они 1 или 2, для паддинга\n",
    "    df['D_87'] = df['D_87'].fillna(0).astype(int) + 1 ## теперь они 1 или 2, для паддинга\n",
    "    return df\n",
    "\n",
    "def preprocess_cat(df):\n",
    "    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "    \n",
    "    trans_table_D_63 =\"CLORXMZna\".maketrans({'C': \"1\", 'L':'2', 'O':'3','R':'4','X':'5','M':'6','Z':'7', 'n':'8', 'a': '9'})\n",
    "    trans_table_D_64 =\"ORUna\".maketrans({'O': \"1\", 'R':'2', 'U':'3', 'n':'4', 'a': '5'})\n",
    "\n",
    "    df['D_63'] = df['D_63'].astype('str').str.translate(trans_table_D_63)        \n",
    "    df['D_64'] = df['D_64'].astype('str').str.translate(trans_table_D_64)\n",
    "    \n",
    "    for col_name in tqdm(cat_features):\n",
    "        df[col_name] = pd.to_numeric(df[col_name])\n",
    "        df[col_name] = df[col_name].fillna(128)\n",
    "        ordered_mapping_from_zero = {e.item(): i + 1 for i, e in enumerate(df[col_name].unique())} # теперь минимальное значение - с 1, для паддинга\n",
    "        df[col_name] = df[col_name].map(ordered_mapping_from_zero)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_df(path):\n",
    "    return df, all_cols\n",
    "\n",
    "def get_emb_size(n_cat):\n",
    "    return min(600, round(1.6 * n_cat ** .56))\n",
    "\n",
    "def create_nunique(df,cat_features):\n",
    "    nunique = {col: df[col].nunique() for col in cat_features}\n",
    "    for key in nunique:\n",
    "        nunique[key] = (nunique[key], get_emb_size(nunique[key]))\n",
    "    return nunique\n",
    "\n",
    "def create_folds(train_df, all_cols, batch_size):\n",
    "    targets = pd.read_csv('amex-default-prediction/train_labels.csv')\n",
    "    targets = targets.set_index('customer_ID')\n",
    "    \n",
    "    grouped = train_df.groupby(\"customer_ID\")\n",
    "    \n",
    "    group_index = {i:g for i,g in enumerate(targets.index.values)}\n",
    "    ys = targets['target'].values\n",
    "\n",
    "    max_len = grouped.count().max().max()\n",
    "\n",
    "    def get_point(index):\n",
    "        group_name = group_index[index]\n",
    "        group = grouped.get_group(group_name)\n",
    "        X = torch.Tensor(group[all_cols].values.astype(np.double))\n",
    "        y = targets.loc[group_name]['target']\n",
    "        X = pad_sequence(X, max_len)\n",
    "        return X, y\n",
    "\n",
    "    def get_batch(indices):\n",
    "        Xs = []\n",
    "        ys = []\n",
    "        for idx in indices:\n",
    "            x,y = get_point(idx)\n",
    "            Xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "        return torch.stack(Xs), torch.ShortTensor(ys)\n",
    "    \n",
    "    def process_fold_index(index, batch_size, folder):\n",
    "        batch_number = 0\n",
    "        elems_count = len(index) // batch_size\n",
    "        for batch_idx in np.array_split(index, elems_count):\n",
    "            X, y = get_batch(batch_idx)            \n",
    "            torch.save(X, f'{folder}/{batch_number}.X.pt')\n",
    "            torch.save(y, f'{folder}/{batch_number}.y.pt')\n",
    "            batch_number+=1\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    for fold_number, (train_index, test_index) in enumerate(skf.split(np.zeros(len(ys)), ys)):\n",
    "        base = f'amex-default-prediction/folds/{fold_number}'\n",
    "        train = f'{base}/train'\n",
    "        test = f'{base}/test'\n",
    "        \n",
    "        os.makedirs(base, exist_ok=True)\n",
    "        os.makedirs(train, exist_ok=True)               \n",
    "        os.makedirs(test, exist_ok=True)\n",
    "        \n",
    "        process_fold_index(train_index, batch_size, train)\n",
    "        process_fold_index(test_index, batch_size, test)\n",
    "            \n",
    "def create_test(test_df, all_cols, batch_size):\n",
    "\n",
    "    grouped = test_df.groupby(\"customer_ID\")\n",
    "    max_len = grouped.count().max().max()\n",
    "    \n",
    "    curr_size = 0\n",
    "    Xs = []\n",
    "    ids = []\n",
    "    batch_number = 0\n",
    "    \n",
    "    base = f'amex-default-prediction/test'\n",
    "    os.makedirs(base, exist_ok=True)\n",
    "    \n",
    "    for name, group in grouped:\n",
    "        X = torch.Tensor(group[all_cols].values.astype(np.double))\n",
    "        X = pad_sequence(X, max_len)\n",
    "        Xs.append(X)\n",
    "        ids.append(name)\n",
    "        curr_size += 1\n",
    "        if curr_size == batch_size:\n",
    "            X_tensor = torch.stack(Xs)\n",
    "            ids = np.asarray(ids)\n",
    "            \n",
    "            torch.save(X_tensor, f'{base}/{batch_number}.X.pt')\n",
    "            np.savetxt(f'{base}/{batch_number}.ids.gz', ids, fmt='%s')\n",
    "            \n",
    "            curr_size = 0\n",
    "            Xs = []\n",
    "            ids = []\n",
    "            batch_number += 1\n",
    "            \n",
    "    if len(Xs) > 0:\n",
    "        X_tensor = torch.stack(Xs)\n",
    "        ids = np.asarray(ids)\n",
    "\n",
    "        torch.save(X_tensor, f'{base}/{batch_number}.X.pt')\n",
    "        np.savetxt(f'{base}/{batch_number}.ids.gz', ids, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16b14bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('amex-default-prediction/train_data.csv')\n",
    "test_df = pd.read_csv('amex-default-prediction/test_data.csv')\n",
    "\n",
    "general = pd.concat([train_df, test_df], ignore_index=True)\n",
    "test_customers = test_df['customer_ID'].values\n",
    "del train_df\n",
    "del test_df\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "651d529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [c for c in list(general.columns) if c not in ['customer_ID','S_2']]\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "bin_features = ['B_31', \"D_87\"]\n",
    "num_features = [col for col in all_cols if col not in cat_features and col not in bin_features]\n",
    "all_cols = cat_features +  bin_features + num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "190c97fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:19<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "general = preprocess_numeric(general, num_features)\n",
    "general = preprocess_cat(general)\n",
    "general = preprocess_binary(general)\n",
    "general=general.reindex(columns=['customer_ID','S_2'] + all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca30a951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "306218df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nunique = create_nunique(general, cat_features + bin_features)\n",
    "\n",
    "with open('nunique.json', 'w+') as fp:\n",
    "    json.dump(nunique, fp)\n",
    "unique_test_customers = np.unique(test_customers)\n",
    "\n",
    "test_df = general[general['customer_ID'].isin(unique_test_customers)]\n",
    "create_test(test_df, all_cols, 4196)\n",
    "del test_df\n",
    "train_df = general[~general['customer_ID'].isin(unique_test_customers)]\n",
    "create_folds(train_df, all_cols, 4196)\n",
    "del train_df\n",
    "del general\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648e478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "524ab698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldStorageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, folder):\n",
    "        self.len = len(os.listdir(folder)) // 2\n",
    "        self.folder = folder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.load(os.path.join(self.folder, f'{idx}.X.pt'), map_location=torch.device('cuda'))\n",
    "        y = torch.load(os.path.join(self.folder, f'{idx}.y.pt'), map_location=torch.device('cuda'))\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a49666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.folder = 'amex-default-prediction/test'\n",
    "        self.size = len(os.listdir(self.folder)) // 2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_path = f'{idx}.X.pt'\n",
    "        ids_path = f'{idx}.ids.gz'\n",
    "        X = torch.load(os.path.join(self.folder, X_path), map_location=torch.device('cuda'))\n",
    "        ids = np.loadtxt(os.path.join(self.folder, ids_path), dtype=np.object)\n",
    "        return X, ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a54ce8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fold_dataloaders():\n",
    "    for fold in range(5):\n",
    "        path = f'amex-default-prediction/folds/{fold}'\n",
    "        train = DataLoader(FoldStorageDataset(f'{path}/train'), batch_size = None)\n",
    "        test = DataLoader(FoldStorageDataset(f'{path}/test'), batch_size = None)\n",
    "        yield train, test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73533b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nunique.json', 'r') as fp:\n",
    "    nunique = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac9dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3be2ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, enc_hid_dim, deep_fcs):\n",
    "        super().__init__()\n",
    "        embedding_projection, embedding_general_output_size = self.get_embedding_projection(nunique)\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList(embedding_projection)\n",
    "        self.inp_size = 220 #emb size + num features len\n",
    "        \n",
    "        self.credits_rnn = torch.nn.GRU(self.inp_size, enc_hid_dim, batch_first=True, bidirectional =True)\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "        self.num_cat_size = 13 # cat + binary become cat\n",
    "        \n",
    "        self.fcs = torch.nn.ModuleList()\n",
    "\n",
    "        inp = enc_hid_dim * 2\n",
    "        for deep in deep_fcs:\n",
    "            out = deep\n",
    "            self.fcs.append(torch.nn.Linear(in_features=inp, out_features=out))\n",
    "            inp = out\n",
    "        \n",
    "    def forward(self, X):\n",
    "        cat_features = X[:,:,:self.num_cat_size]\n",
    "        num_features = X[:,:,self.num_cat_size:]\n",
    "        \n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        splitted = torch.tensor_split(cat_features.long(), cat_features.shape[-1], dim=-1)\n",
    "        emb_to_cat_feature = zip(self.embeddings, splitted)\n",
    "        embeddings = [emb(tensor) for emb, tensor in emb_to_cat_feature]\n",
    "        concatted_emb = torch.squeeze(torch.cat(embeddings, dim=-1))\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            concatted_emb = torch.unsqueeze(concatted_emb, dim=0)\n",
    "        \n",
    "        concatted_input = torch.cat([concatted_emb, num_features], dim=-1)   \n",
    "\n",
    "        rnn_output, hidden_credits_rnn = self.credits_rnn(concatted_input)\n",
    "        rnn_stack = torch.cat([hidden_credits_rnn[0],hidden_credits_rnn[1]], dim=-1)\n",
    "        \n",
    "        x = self.tanh(rnn_stack)\n",
    "        for fc in self.fcs[:-1]:\n",
    "            x = self.relu(fc(x))\n",
    "        x = self.fcs[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def create_embedding_projection(self, cardinality, embed_size, add_missing=True):\n",
    "        add_missing = 1 if add_missing else 0\n",
    "        return torch.nn.Embedding(num_embeddings=cardinality+add_missing, embedding_dim=embed_size, padding_idx=0)\n",
    "\n",
    "    def get_embedding_projection(self, nunique):\n",
    "        embedding_projection = [self.create_embedding_projection(*e) for e in nunique.values()]\n",
    "        embedding_general_output_size = sum([e[1] for e in nunique.values()])\n",
    "        return embedding_projection, embedding_general_output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d2371e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8852191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self,  hidden_size, fcs, weight, reduction):\n",
    "        super().__init__()\n",
    "        self.model = Model(hidden_size, fcs)\n",
    "        self.save_hyperparameters('hidden_size', 'fcs', 'weight', 'reduction')\n",
    "        self.metric = amex_metric_np\n",
    "        weights = torch.Tensor([weight]).to('cuda')\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss(reduction=reduction, pos_weight=weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.squeeze(self.model.forward(x))\n",
    "        loss = self.loss(output, torch.squeeze(y).float()).mean()\n",
    "        return loss    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.sigmoid(torch.squeeze(self.model.forward(x))).detach().cpu().numpy()\n",
    "        y = torch.squeeze(y).detach().cpu().numpy()\n",
    "          \n",
    "        return np.row_stack([output, y])\n",
    "    \n",
    "    def validation_epoch_end(self, test_step_outputs):\n",
    "\n",
    "        catted = np.concatenate(test_step_outputs, axis=1)\n",
    "        \n",
    "        metric = self.metric(catted[0], catted[1])\n",
    "        self.log(\"val_loss\", metric, prog_bar=True)\n",
    "        return {\"val_loss\":  metric}\n",
    "            \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.sigmoid(torch.squeeze(self.model.forward(x))).detach().cpu().numpy()\n",
    "        y = torch.squeeze(y).detach().cpu().numpy()\n",
    "          \n",
    "        return np.row_stack([output, y])\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "\n",
    "        catted = np.concatenate(test_step_outputs, axis=1)\n",
    "        \n",
    "        metric = self.metric(catted[0], catted[1])\n",
    "        self.log(\"test_loss\", metric, prog_bar=True)\n",
    "        return {\"test_loss\":  metric}\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, ids = batch\n",
    "        output = torch.sigmoid(torch.squeeze(self.model.forward(x)))\n",
    "        \n",
    "        return dict([(a, b.item()) for a,b in zip(ids, output.detach().cpu())])\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=0.001)\n",
    "        self.reduce_lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',\n",
    "            factor=0.1,\n",
    "            patience=1,\n",
    "            min_lr=1e-6,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\"optimizer\": self.optimizer,\n",
    "                \"lr_scheduler\":{\n",
    "                    \"scheduler\": self.reduce_lr_on_plateau,# torch.optim.lr_scheduler.ExponentialLR(self.optimizer,gamma=0.9),\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"interval\": \"epoch\"}\n",
    "               }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba673f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar, LearningRateMonitor\n",
    "\n",
    "def fit_model(fold_number, hidden_size, fcs, weight, reduction, train_dataloader, val_dataloader, test_dataloader):\n",
    "    \n",
    "\n",
    "    logger = TensorBoardLogger(save_dir='tb_logs', name=f\"Fcs_{len(fcs)}_hidden_size_{hidden_size}_weight_{weight}_reduction_{reduction}\")    \n",
    "    \n",
    "    module = LightningModule(hidden_size, fcs, weight, reduction)\n",
    "    checkpoint_callback = ModelCheckpoint(dirpath=f'Fcs_{len(fcs)}_hidden_size_{hidden_size}_weight_{weight}_reduction_{reduction}_fold_number_{fold_number}',\n",
    "                                          filename='{epoch}-{val_loss:.4f}',\n",
    "                                          save_top_k=-1)\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor()\n",
    "    \n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", mode='max'),checkpoint_callback, lr_monitor,RichProgressBar(leave=True)] #, \n",
    "    trainer = pl.Trainer(callbacks=callbacks,\n",
    "                         deterministic=True,\n",
    "                         enable_progress_bar=True,\n",
    "                         accelerator=\"gpu\",\n",
    "                         log_every_n_steps=1,\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1)\n",
    "    trainer.fit(model=module, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    \n",
    "    module = LightningModule.load_from_checkpoint(checkpoint_path=checkpoint_callback.best_model_path)\n",
    "    trainer.test(dataloaders=test_dataloader, model=module)\n",
    "    return trainer, module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a382f924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">21/21</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:01 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">17.54it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m21/21\u001b[0m \u001b[38;5;245m0:00:01 • 0:00:00\u001b[0m \u001b[38;5;249m17.54it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7793508171237356     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7793508171237356    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = []\n",
    "for i, (train_dataloader, test_dataloader) in enumerate(create_fold_dataloaders()):\n",
    "    trainer, module = fit_model(i, 256, [256,128,64,32,16,8,1], 1, 'mean', train_dataloader, test_dataloader, test_dataloader)\n",
    "    outputs.append((trainer, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc29eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8198ac61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Predicting</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">221/221</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:30 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">7.33it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mPredicting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m221/221\u001b[0m \u001b[38;5;245m0:00:30 • 0:00:00\u001b[0m \u001b[38;5;249m7.33it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = []\n",
    "for path in [\n",
    "             'Fcs_7_hidden_size_256_weight_1_reduction_mean_fold_number_3/epoch=19-val_loss=0.7790.ckpt',\n",
    "             'Fcs_7_hidden_size_256_weight_1_reduction_mean_fold_number_4/epoch=17-val_loss=0.7797.ckpt'             \n",
    "            ]:\n",
    "    module = LightningModule.load_from_checkpoint(checkpoint_path=path)\n",
    "    predictions =  create_trainer().predict(dataloaders=DataLoader(TestDataset(), batch_size=None), model=module)\n",
    "    predictions = {k: v for d in predictions for k, v in d.items()}\n",
    "    df = pd.DataFrame(predictions.items(), columns=['customer_ID', 'prediction'])\n",
    "    values.append(df['prediction'].values)\n",
    "df['prediction'] = sum(values) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for trainer, module in outputs:\n",
    "    predictions =  create_trainer().predict(dataloaders=DataLoader(TestDataset(), batch_size=None), model=module)\n",
    "    predictions = {k: v for d in predictions for k, v in d.items()}\n",
    "    df = pd.DataFrame(predictions.items(), columns=['customer_ID', 'prediction'])\n",
    "    values.append(df['prediction'].values)\n",
    "df['prediction'] = sum(values) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41e2dee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.065661\n",
       "1         0.000372\n",
       "2         0.029844\n",
       "3         0.304707\n",
       "4         0.876459\n",
       "            ...   \n",
       "924616    0.012988\n",
       "924617    0.806117\n",
       "924618    0.566064\n",
       "924619    0.352699\n",
       "924620    0.086094\n",
       "Name: prediction, Length: 924621, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d22b21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
